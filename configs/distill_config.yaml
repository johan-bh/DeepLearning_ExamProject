teacher_model:
  name: "syvai/hviske-v2"
  device: 0
  fp16: false

student_model:
  name: "openai/whisper-large-v3-turbo"
  device: 0
  fp16: false

dataset:
  name: "alexandrainst/coral"
  train_split: "train"
  eval_split: "val"
  train_samples: 200
  eval_samples: 50
  num_proc: 1

training:
  output_dir: "models/distilled-whisper-turbo"
  batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  num_train_epochs: 3
  fp16: false
  logging_steps: 25
  temperature: 2.0  # Temperature for knowledge distillation
  alpha: 0.5  # Weight for distillation loss (1-alpha for regular loss) 