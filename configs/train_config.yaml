defaults:
  - _self_

# Training parameters
training:
  output_dir: "models/whisper-large-v3-turbo-finetuned_50k"
  num_train_epochs: 3
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 1e-5
  weight_decay: 0.01
  max_steps: -1
  fp16: false
  bf16: true
  warmup_steps: 500
  save_steps: 1000
  eval_steps: 1000
  logging_steps: 500
  gradient_clip_val: 1.0


# Model parameters
model:
  name: "openai/whisper-large-v3"
  device: 0
  fp16: false

# Dataset parameters
dataset:
  name: "alexandrainst/coral"
  data_dir: "huge_subset/data"
  config: null
  sampling_rate: 16000
  num_proc: 30
  batch_size_per_proc: 8


# Deprecated
  language: "Danish"
  train_split: "train"
  eval_split: "val"
  train_size: 20000
  val_size: 500
  seed: 42



  download:
    output_dir: "huge_subset"
    train_size: 50000
    val_size: 1000

eval:
  num_samples: 5
device: "cuda"