defaults:
  - _self_

# Training parameters
training:
  output_dir: "models/whisper-large-v3-turbo"
  num_train_epochs: 1           # Increased epochs for better convergence with larger data
  batch_size: 16                   # Moderate batch size, adjust based on GPU memory
  gradient_accumulation_steps: 2  # Simulate a larger batch size (effective batch size = 16)
  learning_rate: 1e-5             # Lowered learning rate for more stable updates
  weight_decay: 0.01              # Standard weight decay for regularization
  max_steps: -1                   # Unlimited steps, controlled by epochs
  fp16: false                      # Enable mixed precision for faster training and reduced memory usage
  bf16: true                     # Disable bf16; use fp16 if supported by hardware
  warmup_steps: 500               # Increased warmup steps for gradual learning rate increase
  # warmup_ratio: 0.1             # Removed to avoid conflict with warmup_steps
  save_steps: 1000                # Save model checkpoints every 1000 steps
  eval_steps: 1000                # Evaluate every 1000 steps
  logging_steps: 500              # Increased logging frequency for better monitoring
  gradient_clip_val: 1.0          # Prevent exploding gradients by clipping


# Model parameters
model:
  name: "openai/whisper-large-v3"
  device: 0
  fp16: false

# Dataset parameters
dataset:
  name: "alexandrainst/coral"
  config: null
  sampling_rate: 16000
  num_proc: 30
  batch_size_per_proc: 8
  language: "Danish"
  train_split: "train"
  eval_split: "val"
  train_size: 10000
  val_size: 1000
  seed: 42

eval:
  num_samples: 5
device: "cuda"  # Use "cuda" for GPU or "cpu" for CPU