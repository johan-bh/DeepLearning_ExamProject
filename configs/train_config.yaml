defaults:
  - _self_

# Training parameters
training:
  output_dir: "models/whisper-large-v3-turbo"
  num_train_epochs: 2
  batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_steps: -1
  fp16: true
  save_steps: 100
  eval_steps: 100
  logging_steps: 100

# Model parameters
model:
  name: "openai/whisper-large-v3-turbo"
  freeze_encoder: true

# Dataset parameters
dataset:
  name: "alexandrainst/nst-da"
  config: null
  sampling_rate: 16000
  num_proc: 1
  train_split: "train"
  eval_split: "test"