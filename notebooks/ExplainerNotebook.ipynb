{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish ASR: Fine-tuning Whisper\n",
    "\n",
    "This notebook documents our pipeline for fine-tuning and knowledge distilling (KD) Whisper-Large-v3-Turbo on Danish ASR data. It was created for reproducibility purposes, providing a detailed walkthrough of our experimental setup and implementation.\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "Our experiments were conducted on two different GPU setups:\n",
    "- Preliminary work: NVIDIA A100 (80GB)\n",
    "- Final models: NVIDIA H200 (141GB)\n",
    "\n",
    "To reproduce our results with the provided configurations, you'll need similar high-end GPU resources, particularly for:\n",
    "- Batch sizes (32 for fine-tuning, 24 for distillation)\n",
    "- Mixed precision training (bfloat16)\n",
    "- Model sizes (Whisper-Large-v3 (Hviske-v2): ~1.5B parameters & Whisper-Large-v3-Turbo 809M parameters))\n",
    "\n",
    "If using different GPU hardware, you may need to adjust:\n",
    "- `batch_size` in train_config.yaml\n",
    "- `per_device_train_batch_size` in distill_config.yaml\n",
    "- Precision settings (`fp16`/`bf16`) - perhaps using 8 or 4 bit quantization\n",
    "- Gradient accumulation steps and or using gradient checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Overview and Execution\n",
    "\n",
    "Before running the experiments, configure your settings in:\n",
    "1. `configs/train_config.yaml`: Fine-tuning parameters and dataset paths\n",
    "2. `configs/distill_config.yaml`: Knowledge distillation settings\n",
    "3. `configs/baseline_config.yaml`: Evaluation parameters for benchmark testing\n",
    "\n",
    "The pipeline consists of the following steps:\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "- Downloads the ASR dataset(s): `alexandrainst/coral`\n",
    "- Saves preprocessed data locally\n",
    "- Ensures consistent format for training\n",
    "\n",
    "### 2. Training (2 Approaches)\n",
    "\n",
    "- Both approaches use bfloat16 precision\n",
    "- Fine-tuning uses batch size of 32\n",
    "- Knowledge distillation uses batch size of 24\n",
    "- Models are saved to the specified output directory\n",
    "\n",
    "### 3. Evaluation\n",
    "- Comprehensive testing on multiple Danish benchmarks\n",
    "- Detailed performance analysis across datasets\n",
    "- Results saved in `evaluation/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for simplicity and ease of the reader we have included python blocks to print the contents of the respective files. Otherwise, please refer to the actual python files in their respective locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation and Download\n",
    "\n",
    "Due to the large size of Danish ASR datasets (several hundred GBs), we've separated the dataset downloading and preprocessing steps. The `download_dataset.py` script handles this through configuration parameters in `train_config.yaml`:\n",
    "\n",
    "```yaml\n",
    "dataset:\n",
    "  download:\n",
    "    output_dir: \"huge_subset\"      # Directory where the dataset will be saved\n",
    "    train_size: 50000             # Number of training samples to download\n",
    "    val_size: 1000               # Number of validation samples to download\n",
    "```\n",
    "\n",
    "To download a subset of the dataset:\n",
    "```bash\n",
    "python src/data/download_dataset.py\n",
    "```\n",
    "\n",
    "**Important Notes:**\n",
    "- The download process saves the data locally for faster access during training\n",
    "- We avoid streaming the full dataset during training because streaming + audio column casting can cause memory issues\n",
    "- We also download the dataset for consistency across runs, it's very tedious to apply shuffling on these large datasets\n",
    "- The downloaded data is saved in `{output_dir}/data/train` and `{output_dir}/data/val`\n",
    "- Make sure you have sufficient disk space before downloading (each audio sample can be several MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the download_dataset.py script\n",
    "from pathlib import Path\n",
    "\n",
    "def show_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "print(\"download_dataset.py:\")\n",
    "show_file('../src/data/download_dataset.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "The `data_loader.py` script provides a consistent interface for loading locally stored datasets across different computing environments (local machines, VM instances, etc.). The actual data preprocessing happens in the training scripts.\n",
    "\n",
    "```python\n",
    "# Example from data_loader.py\n",
    "def load_dataset(cfg):\n",
    "    \"\"\"\n",
    "    Load dataset from local directory with consistent path handling\n",
    "    Args:\n",
    "        cfg: Configuration containing dataset parameters\n",
    "    Returns:\n",
    "        DatasetDict: Dataset with 'train' and 'validation' splits\n",
    "    \"\"\"\n",
    "    data_dir = Path(cfg.dataset.data_dir)\n",
    "    return DatasetDict({\n",
    "        'train': load_from_disk(data_dir / 'train'),\n",
    "        'validation': load_from_disk(data_dir / 'val')\n",
    "    })\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- The loader ensures consistent path handling across different OS environments\n",
    "- Audio preprocessing (resampling, feature extraction) is handled in the training scripts\n",
    "- The loader expects data in the structure created by `download_dataset.py`:\n",
    "  ```\n",
    "  huge_subset/\n",
    "  └── data/\n",
    "      ├── train/\n",
    "      └── val/\n",
    "  ```\n",
    "\n",
    "The actual data preprocessing pipeline (audio transformations, batching, etc.) is implemented in the respective training scripts (`finetune_whisper.py`, `knowledge_distil.py`) to maintain flexibility for different training approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data_loader.py:\")\n",
    "show_file('../src/data/data_loader.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning\n",
    "\n",
    "The `finetune_whisper.py` script fine-tunes Whisper on Danish ASR data. The hyperparameters are controlled through `train_config.yaml`:\n",
    "\n",
    "```yaml\n",
    "# Training configuration\n",
    "training:\n",
    "  output_dir: \"models/whisper-large-v3-turbo-finetuned_50k\"  # Where to save the model\n",
    "  num_train_epochs: 3                                         # Number of training epochs\n",
    "  batch_size: 32                                             # Batch size per GPU\n",
    "  gradient_accumulation_steps: 1                             # Accumulation for larger effective batch\n",
    "  learning_rate: 3e-5                                        # Learning rate\n",
    "  weight_decay: 0.01                                         # Weight decay for regularization\n",
    "  bf16: true                                                # Use bfloat16 precision\n",
    "  warmup_steps: 500                                         # Learning rate warmup\n",
    "  save_steps: 1000                                          # Save checkpoint every N steps\n",
    "  eval_steps: 1000                                          # Evaluate every N steps\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  name: \"openai/whisper-large-v3\"                           # Base model to fine-tune\n",
    "  device: 0                                                 # GPU device ID\n",
    "  fp16: false                                              # Don't use float16 precision\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  name: \"alexandrainst/coral\"                              # Dataset identifier\n",
    "  data_dir: \"huge_subset/data\"                             # Path to local dataset\n",
    "  sampling_rate: 16000                                     # Audio sampling rate\n",
    "  num_proc: 30                                             # Number of preprocessing workers\n",
    "  batch_size_per_proc: 8                                   # Batch size per worker\n",
    "```\n",
    "\n",
    "The script:\n",
    "1. Loads the pretrained Whisper model with the specified configuration\n",
    "2. Sets up training with the defined hyperparameters\n",
    "3. Fine-tunes on the Danish dataset\n",
    "4. Saves checkpoints and the final model to the specified `output_dir`\n",
    "\n",
    "**Note:** Due to the large size of the fine-tuned models (several GBs), they are not included in the repository. The trained models can be accessed via our [Google Drive link](https://drive.google.com/drive/folders/1AoEGmsw_cjO7eRFs3s6dPXd2oSiWj7cO?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finetune_whisper.py:\")\n",
    "show_file('../src/models/finetune_whisper.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Distillation\n",
    "\n",
    "The `knowledge_distil.py` script performs knowledge distillation from a teacher to a student Whisper model. The hyperparameters are controlled through `distill_config.yaml`:\n",
    "\n",
    "```yaml\n",
    "# Teacher model configuration\n",
    "teacher_model:\n",
    "  name: \"syvai/hviske-v2\"                                  # Teacher model to distill from\n",
    "  device: 1                                                # GPU device ID for teacher\n",
    "  fp16: false                                             # Don't use float16 precision\n",
    "\n",
    "# Student model configuration\n",
    "student_model:\n",
    "  name: \"openai/whisper-large-v3-turbo\"                   # Student model to train\n",
    "  device: 1                                               # GPU device ID for student\n",
    "  fp16: false                                            # Don't use float16 precision\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  name: \"alexandrainst/coral\"                            # Dataset identifier\n",
    "  data_dir: \"huge_subset/data\"                           # Path to preprocessed dataset\n",
    "  num_proc: 5                                            # Number of preprocessing workers\n",
    "\n",
    "# Training configuration\n",
    "training:\n",
    "  output_dir: \"models/distilled-whisper-turbo-large_subset\"  # Where to save the model\n",
    "  per_device_train_batch_size: 24                            # Batch size per GPU\n",
    "  per_device_eval_batch_size: 24                             # Evaluation batch size\n",
    "  gradient_accumulation_steps: 1                             # Accumulation for larger batch\n",
    "  learning_rate: 3e-5                                        # Learning rate\n",
    "  num_train_epochs: 1                                        # Number of training epochs\n",
    "  bf16: true                                                # Use bfloat16 precision\n",
    "  temperature: 2                                             # Softmax temperature\n",
    "  alpha: 0.7                                                # KD loss weight\n",
    "\n",
    "# LoRA parameters (implemented but not used in final experiments)\n",
    "lora:\n",
    "  rank: 16\n",
    "  alpha: 64\n",
    "  dropout: 0.05\n",
    "  target_modules: [\n",
    "    \"q_proj\", \"v_proj\", \"k_proj\",\n",
    "    \"out_proj\", \"fc1\", \"fc2\"\n",
    "  ]\n",
    "```\n",
    "\n",
    "The script:\n",
    "1. Loads both teacher and student models with their respective configurations\n",
    "2. Uses the preprocessed dataset from the specified `data_dir`\n",
    "3. Sets up knowledge distillation training with the defined hyperparameters\n",
    "4. Saves checkpoints and the final distilled model to the specified `output_dir`\n",
    "\n",
    "**Note:** Like the fine-tuned models, the distilled models are also available via our [Google Drive link](insert_link_here) due to their large size.\n",
    "\n",
    "**Key Concepts:**\n",
    "- `temperature`: Controls the softness of the teacher's predictions\n",
    "- `alpha`: Balances between distillation and task-specific losses\n",
    "- `data_dir`: Points to the preprocessed dataset created by `download_dataset.py`\n",
    "\n",
    "**Future Work:**\n",
    "While we implemented LoRA (Low-Rank Adaptation) for memory-efficient fine-tuning, this approach was not included in the final report or experiments. The implementation remains in the codebase for future exploration and comparison with our current approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the knowledge_distil.py script\n",
    "from pathlib import Path\n",
    "\n",
    "def show_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "print(\"knowledge_distil.py:\")\n",
    "show_file('../src/KD/knowledge_distil.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Our evaluation pipeline consists of two main components:\n",
    "\n",
    "The `finetuned_test.py` script evaluates models on the Coral dataset (our primary fine-tuning target):\n",
    "- Detailed demographic analysis (age, gender, dialect)\n",
    "- Word Error Rate (WER) and Character Error Rate (CER)\n",
    "- Example transcriptions for qualitative analysis\n",
    "- Results saved in `evaluation/finetuned/`\n",
    "\n",
    "The `baseline2.py` script tests model generalization across different data distributions using three Danish ASR benchmarks:\n",
    "1. Mozilla Common Voice 17.0\n",
    "2. NST (Danish)\n",
    "3. Google/FLEURS Danish\n",
    "\n",
    "**Note:** Results are organized in separate directories:\n",
    "- `evaluation/finetuned/`: Target dataset (Coral) results\n",
    "- `evaluation/benchmarking/`: Cross-dataset benchmark results (we also test our trained models here, but on acessory datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation on target dataset:\")\n",
    "show_file('../src/models/finetuned_test.py')\n",
    "\n",
    "print(\"\\nBenchmark testing across datasets:\")\n",
    "show_file('../src/models/baseline2.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
